{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CovarianceMatrices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clacri/iucr2020_compschool/blob/main/randy_covariance/CovarianceMatrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48a9589"
      },
      "source": [
        "# Understanding and manipulating covariance matrices\n",
        "\n",
        "Randy J Read, Cambridge Institute for Medical Research, University of Cambridge, rjr27@cam.ac.uk"
      ],
      "id": "f48a9589"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Py1YeyVj6nQ"
      },
      "source": [
        "## Preamble\n",
        "\n",
        "The following background is assumed: an understanding of how to multiply matrices and vectors, the concept of an inverse matrix, familiarity with the ideas of eigenvalues and eigenvectors. Some basic knowledge of Python will help.\n",
        "\n",
        "The following mathematical notation is used: italic lower case for variables representing real numbers (*e.g.* $x$), bold lower case for both complex numbers ($\\mathbf{z}$) and vectors ($\\mathbf{v}$) and bold upper case for matrices ($\\mathbf{A}$). A vector should be understood to be a column vector by default (*e.g.* $\\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}$)"
      ],
      "id": "1Py1YeyVj6nQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5216e724"
      },
      "source": [
        "# Set up, including importing the libraries we will need\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from numpy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats"
      ],
      "id": "5216e724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5b53c5"
      },
      "source": [
        "Before we start looking at various flavours of normal distributions (which are described by variances or, more generally, covariance matrices), let's review the Central Limit Theorem, which underpins the importance of normal distributions."
      ],
      "id": "3a5b53c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a2f501"
      },
      "source": [
        "## The Central Limit Theorem and normal distributions"
      ],
      "id": "f8a2f501"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c19f941"
      },
      "source": [
        "Normal distributions appear everywhere in experimental science. This is because most of the things we observe result from adding up a variety of sources of variation: different contributions to the signal and to the error in the signal. We can think of the observation as a sum of random variables describing these different contributions to signal and error. The Central Limit Theorem tells us that, as the number of random variables contributing to a sum increases, the probability distribution of the sum approaches ever closer to a normal distribution.\n",
        "\n",
        "A normal distribution for a single variable (often called a Gaussian or referred to as the bell-shaped curve) is described by two parameters: the mean (expected value, *i.e.* probability-weighted average over all possible values) and the variance (or its square root, the standard deviation). According to the Central Limit Theorem, if the random variables are independent of one another (uncorrelated), the expected value of the distribution of their sum is the sum of the expected values of the individual random variables, and the variance of the distribution of the sum is the sum of the individual variances:\n",
        "\n",
        "$s = \\sum_j x_j$, where the individual $x_j$ are different random variables in the sum\n",
        "\n",
        "$\\mu = \\left<s\\right> = \\sum_j \\left<x_j\\right>$, where $\\left<s\\right>$ indicates the probability-weighted average, i.e. $\\left<s\\right> = \\int_s p(s)\\,s\\,ds$\n",
        "\n",
        "$\\sigma^2 = \\sigma^2_s = \\sum_j \\sigma^2_{x_j}$, where $\\sigma^2_{x_j} = \\left<(x_j - \\left<x_j\\right>)^2\\right>$ or the mean-square deviation from the mean.\n",
        "\n",
        "The expected value and variance define the normal distribution:\n",
        "\n",
        "$p(s) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp(-\\frac{(s - \\mu)^2}{2 \\sigma^2})$\n",
        "\n",
        "There are a few conditions to this. First, there must be a sufficient number of random variables in the sum, though as we will see even as few as 5 or 10 can result in something reasonably close to a normal distribution. Second, none of the variables can dominate the distribution. Of course, the means and variances of the individual random variables have to be defined and finite.\n",
        "\n",
        "It's easy to demonstrate the Central Limit Theorem and its limitations with some simple numerical experiments. For instance, a top-hat function (a flat function that is only non-zero between two defined values) doesn't look anything like a normal distribution. We can use a top-hat function to represent the probability distribution of numbers drawn randomly from the range $0 \\leq x < 1$."
      ],
      "id": "8c19f941"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36b45391"
      },
      "source": [
        "plt.step([-1,0,1,2],[0,0,1,0])\n",
        "plt.show"
      ],
      "id": "36b45391",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a611abc7"
      },
      "source": [
        "Note that the area under the top-hat function is one: in a distribution that covers the probability of all possible outcomes, the total probability has to add up to one, because something has to happen!\n",
        "\n",
        "We can generate a set of numbers from that distribution and look at the histogram of their values to make sure they're being generated correctly."
      ],
      "id": "a611abc7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75b99122"
      },
      "source": [
        "nx = 5000\n",
        "x = np.random.random_sample(nx)\n",
        "plt.hist(x,bins=20,density=True)\n",
        "plt.step([-1,0,1,2],[0,0,1,0])\n",
        "plt.show"
      ],
      "id": "75b99122",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1462020"
      },
      "source": [
        "Let's compare the distribution of sums of random numbers in the range 0 to 1 with the normal distribution. The expected value for each random number is $\\frac{1}{2}$, so the sum of the expected values is the number of samples divided by 2. The variance of the distribution for each random number is given by\n",
        "\n",
        "$\\sigma^2 = \\int_0^1\\,(x-\\frac{1}{2})^2\\,dx = \\frac{1}{12}$ \n",
        "\n",
        "so the variance of the sum is obtained by dividing the number of samples by 12.\n",
        "\n",
        "Even a sample of 10 numbers drawn from the same top-hat distribution is enough for the normal distribution to be a good approximation for their sum."
      ],
      "id": "e1462020"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84305b25"
      },
      "source": [
        "nsamp = 10\n",
        "mu = nsamp / 2.\n",
        "variance = nsamp / 12. \n",
        "sigma = np.sqrt(variance)\n",
        "ndata = 5000\n",
        "data = []\n",
        "for i in range(ndata):\n",
        "    data.append(np.sum(np.random.random_sample(nsamp)))\n",
        "plt.hist(data,bins=20,density=True)\n",
        "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
        "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
        "plt.show()"
      ],
      "id": "84305b25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NksZAyTgHZM5"
      },
      "source": [
        "### Exercise on effect of sample size on the Central Limit Theorem"
      ],
      "id": "NksZAyTgHZM5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MteloJNCHtlt"
      },
      "source": [
        "Look at the effect of changing the number of samples in the distribution above by changing *nsamp*. Include 1 and 2 in the values you try!"
      ],
      "id": "MteloJNCHtlt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGw5tJ3SJaBN"
      },
      "source": [
        "## Breaking the assumptions of the Central Limit Theorem"
      ],
      "id": "GGw5tJ3SJaBN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6cf7c8"
      },
      "source": [
        "However, if we replace even 1 of those 10 random numbers with a random number drawn from a wider range (and adjust the mean and variance appropriately), the distribution is no longer normal. What we get is a compromise between the flat distribution for a single number drawn from the broader distribution (shown as a top-hat function shifted by the mean contribution from the smaller variables) and the normal distribution from the rest of the smaller terms. We would have to add many samples from the narrower distribution to make the normal distribution a good approximation again. (You can play with this by changing *nsmall*. Try changing the breadth of the \"big\" distribution too!)"
      ],
      "id": "ce6cf7c8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fca755c2"
      },
      "source": [
        "nsmall = 9\n",
        "small = 1\n",
        "nbig = 1\n",
        "big = 10\n",
        "mu = (nbig*big + nsmall*small) / 2.\n",
        "variance = (nbig*big**2 + nsmall*small**2) / 12.\n",
        "sigma = np.sqrt(variance)\n",
        "ndata = 5000\n",
        "data = []\n",
        "for i in range(ndata):\n",
        "    data.append(np.sum(big  *np.random.random_sample(nbig))\n",
        "              + np.sum(small*np.random.random_sample(nsmall)))\n",
        "plt.hist(data,bins=20,density=True)\n",
        "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
        "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
        "if (nbig == 1): # Only plot top-hat for case of 1 sample from the broad distribution\n",
        "    plt.step([mu-big/2-1, mu-big/2, mu+big/2, mu+big/2+1],\n",
        "             [0, 0, 1/big, 0])\n",
        "plt.show()"
      ],
      "id": "fca755c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a7122f"
      },
      "source": [
        "What is going on here is that every time we add a new random variable, the distribution so far is broadened by a convolution with the distribution of the new variable. Hopefully you've encountered convolutions before, but if not you can think of a convolution as smearing one function out by the shape of the other function. In the limit (with variables that have similar variances), this tends to a normal distribution regardless of the shapes of the individual distributions. Here, the top-hat distribution for the big variable is smeared out by the much smaller variation from the approximate normal distribution of the sum of the remaining terms, and that's not enough to lose its flat top. Nonetheless, if you add enough of the smaller contributions (say, 100 of them in this case), you will get something close to a normal distribution again.\n",
        "\n",
        "Note that the Central Limit Theorem can be generalised to higher dimensions (the multivariate normal distribution relating collections of real numbers), complex numbers (the complex normal distribution) or both (the multivariate complex normal distribution). We'll be looking at these, particularly the multivariate versions that require a covariance matrix to describe them."
      ],
      "id": "02a7122f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTFrVwwFKmp7"
      },
      "source": [
        "### Exercise on effect of a dominant variable in the Central Limit Theorem\n"
      ],
      "id": "RTFrVwwFKmp7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1kC-JSkKp_Q"
      },
      "source": [
        "See what happens as you increase *nbig* in the simulation above to larger numbers, or if you reduce *nsmall*."
      ],
      "id": "o1kC-JSkKp_Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb2bda48"
      },
      "source": [
        "## The multivariate normal distribution"
      ],
      "id": "cb2bda48"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9da4d75"
      },
      "source": [
        "An alternative way of expressing the normal distribution makes the generalisation to the multivariate case easier to follow:\n",
        "\n",
        "$p(x) = (2 \\pi \\Sigma)^{-1/2} \\exp(-\\frac{1}{2}(x - \\mu) \\Sigma^{-1} (x - \\mu))$, where $\\Sigma = \\sigma^2 = \\left<(x-\\mu)(x-\\mu)\\right>$\n",
        "\n",
        "Turning this into an expression for the multivariate normal distribution simply involves replacing the variables $x$ and $\\mu$ by vectors and replacing the scalar $\\Sigma$ by a covariance matrix.\n",
        "\n",
        "$p(\\mathbf{x}) = (|2 \\pi\\boldsymbol{\\Sigma}|^{-1/2} \\exp(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}))$\n",
        "\n",
        "where $|2 \\pi\\mathbf\\Sigma|$ indicates the determinant of $2 \\pi$ times the covariance matrix (which is a nice shorthand that is equal to the product of the determinant of the matrix and $2 \\pi$ raised to the order of the matrix) and a superscript $T$ indicates the transpose of the vector. In this equation, $\\mathbf{x}$ and $\\boldsymbol{\\mu}$ are column vectors: the product $\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})$ is therefore also a column vector so that when it is premultiplied by the row vector $(\\mathbf{x} - \\boldsymbol{\\mu})^T$, giving the inner or dot product of the vectors, the result is a real number.\n",
        "\n",
        "Note that we can also generalise the expression for the covariance matrix from the expression for the 1D variance of the normal distribution. Reversing the row and column vectors gives us the outer product, *i.e.* a matrix. This way of expressing the covariance matrix will turn out to be very useful later!\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \\left<(\\mathbf{x} - \\boldsymbol{\\mu})(\\mathbf{x} - \\boldsymbol{\\mu})^T\\right>$ = \n",
        "$\\begin{bmatrix} \n",
        "\\left<(x_1 - \\mu_1)(x_1 - \\mu_1)\\right> & \\cdots & \\left<(x_1 - \\mu_1)(x_n - \\mu_n)\\right> \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\left<(x_n - \\mu_n)(x_1 - \\mu_1)\\right> & \\cdots & \\left<(x_n - \\mu_n)(x_n - \\mu_n)\\right> \n",
        "\\end{bmatrix}$\n",
        "\n",
        "The diagonal elements of this matrix are the variances of the individual variables, and the off-diagonal elements are the covariances, which are related to correlations. It's important to note that the covariances are calculated without assuming values for any other variables to which they might both be correlated. In some circumstances, you might have to integrate over all possible values of such *nuisance* variables.\n",
        "\n",
        "We can get an intuitive idea of how this works by looking at a bivariate (two-dimensional) distribution, replacing the vector $\\mathbf{x}$ with the vector $\\begin{bmatrix}x\\\\y\\end{bmatrix}$. The triple product inside the exponential (sometimes called the quadratic form) can be expanded out into a relatively simple form:\n",
        "\n",
        "$(\\mathbf{x} - \\boldsymbol{\\mu})^T\\,\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) = \n",
        "\\begin{bmatrix} (x - \\mu_x) & (y - \\mu_y) \\end{bmatrix}\n",
        "\\begin{bmatrix} \\sigma^2_x & \\sigma_{xy} \\\\ \\sigma_{xy} & \\sigma^2_y \\end{bmatrix}^{-1}\n",
        "\\begin{bmatrix} (x - \\mu_x) \\\\ (y - \\mu_y) \\end{bmatrix}$, \n",
        "\n",
        "where $\\sigma_{xy} = \\left<(x - \\mu_x)(y - \\mu_y)\\right>$\n",
        "\n",
        "To see the relationship between  covariance and  correlation, note that the definition of the correlation between $x$ and $y$ is $\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}$. If we normalised $x$ and $y$ (and their expected values) by dividing by their standard deviations, the covariance matrix relating them would turn into a correlation matrix: $\\begin{bmatrix} 1 & \\rho_{xy} \\\\ \\rho_{xy} & 1 \\end{bmatrix}$.\n",
        "\n",
        "How the probability distribution depends on the standard deviations and the correlation coefficient can be explored by viewing it in a contour plot below."
      ],
      "id": "c9da4d75"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89dc408f"
      },
      "source": [
        "# Start by defining functions to return a 2D grid for the contour plot and to \n",
        "# annotate with arrows.\n",
        "\n",
        "def make_grid(center, sigmas, ndat=60):\n",
        "    x = np.linspace(center[0]-3*sigmas[0], center[0]+3*sigmas[0], ndat)\n",
        "    y = np.linspace(center[1]-3*sigmas[1], center[1]+3*sigmas[1], ndat)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    pos = np.dstack((X, Y))\n",
        "    return x, y, pos\n",
        "\n",
        "def annotate_arrow(ax, start, end):\n",
        "    ax.annotate(\"\",\n",
        "            xy = end, # Square root to show standard deviation\n",
        "            xycoords='data',\n",
        "            xytext=start, \n",
        "            textcoords='data',\n",
        "            arrowprops=dict(arrowstyle=\"->\",shrinkA=0,shrinkB=0))"
      ],
      "id": "89dc408f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b1cf6c1"
      },
      "source": [
        "sigmax = 1.5\n",
        "varx = sigmax**2\n",
        "sigmay = 1\n",
        "vary = sigmay**2\n",
        "corrxy = -0.5\n",
        "covarxy = sigmax * sigmay * corrxy\n",
        "Sigma = np.array([[varx, covarxy],[covarxy, vary]])\n",
        "mu = np.array([0,0])\n",
        "\n",
        "# Now do a contour plot of the distribution\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect(\"equal\")\n",
        "fig.set_dpi(100)\n",
        "# Add the eigenvectors showing the principal axes\n",
        "# Note that eigenvectors are in the columns, and the eigenvalues are the \n",
        "# variances of those components\n",
        "w,v = linalg.eig(Sigma)\n",
        "end = mu + np.sqrt(w[0])*v[:,0] # Square root to show standard deviation\n",
        "annotate_arrow(ax, mu, end)\n",
        "end = mu + np.sqrt(w[1])*v[:,1]\n",
        "annotate_arrow(ax, mu, end)\n",
        "x, y, pos = make_grid(np.array((0,0)),np.array((sigmax,sigmay))) # 2D grid for plotting\n",
        "bivardist = stats.multivariate_normal(mu,Sigma) # Define the distribution\n",
        "plt.contourf(x,y,bivardist.pdf(pos)) # Plot over grid\n",
        "plt.show()"
      ],
      "id": "0b1cf6c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5de367c"
      },
      "source": [
        "The two arrows on the contour plot are the eigenvectors of the covariance matrix, scaled by the square root of the corresponding eigenvalue to show a one-standard-deviation shift.\n",
        "\n",
        "When we go to higher dimensions than two, it may seem surprising that all we need is pairwise cross-terms, because there is higher-order behaviour in the joint distribution: the probability of values taken by two of the variables will depend on the values of other variables to which both are correlated. However, this higher-order information emerges from the step where the covariance matrix is inverted."
      ],
      "id": "c5de367c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSQ5MC1NkVhX"
      },
      "source": [
        "### Exercise on varying correlation and standard deviations"
      ],
      "id": "RSQ5MC1NkVhX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHHUVtinka0K"
      },
      "source": [
        "Make a copy of the cell above (because you'll want to refer to the original contour plot later). Try changing the parameters in your copy, *i.e.* the two standard deviations and the correlation coefficient, to see how these affect the distribution."
      ],
      "id": "dHHUVtinka0K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d021a10d"
      },
      "source": [
        "## Deriving distributions of subsets of variables in a multivariate normal distribution"
      ],
      "id": "d021a10d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35d5679b"
      },
      "source": [
        "Apart from being very common, multivariate normal distributions have many useful properties that allow us to perform easy manipulations. For instance, we frequently know the joint distribution of a number of variables (expressed as a multivariate normal distribution) but what we really need is the joint distribution of a smaller number of them. This comes in two flavours: either we don't know or care about the values of the subset of variables we want to leave out, or we do know them and we want to know the effect of fixing the ones we know on the rest of the variables. The first flavour corresponds to a marginal probability distribution and the second to a conditional probability distribution. In both cases, the new distribution is also a normal distribution, but one with reduced dimensions."
      ],
      "id": "35d5679b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2faa2e1"
      },
      "source": [
        "### Marginal probability distributions"
      ],
      "id": "e2faa2e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705cb435"
      },
      "source": [
        "To obtain a probability distribution that doesn't depend on the values of any variables left out, in principle we have to integrate over all possible values of the variables we're leaving out, weighting the integral by the probabilities of those values. (This is called a marginal probability distribution, because you can think of the integration over one or more variables as projecting the higher-dimensional distribution onto the corresponding margin of the sample space.) However, remember that we constructed the covariance matrix by looking only at pairs of variables, and each covariance element was derived assuming that we didn't know the values of nuisance variables to which both might be correlated. We could, in fact, have used the same results to build the covariance matrix of the smaller subset of variables. This means that we can derive the marginal probability distribution simply by discarding the variables we don't want, including the corresponding rows and columns of the covariance matrix and the elements of the expected value vector.\n",
        "\n",
        "Here's a simple example where we derive a univariate normal distribution from the bivariate distribution illustrated above. In the contour plot above, the probabilities of different values of $x$ depend on the value of $y$, with negative values of $y$ leading to more positive values of $x$ and *vice versa*. However, if we imagine projecting the distribution down the $y$ axis, we can see that the overall average value of $x$ is zero. The following shows how we can get this result simply by discarding the terms relating to the $y$ variable from above."
      ],
      "id": "705cb435"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9e92500"
      },
      "source": [
        "mux = 0\n",
        "sigmax = 1.5\n",
        "x = np.linspace(mux - 3*sigmax, mux + 3*sigmax, 100)\n",
        "plt.plot(x, stats.norm.pdf(x, mux, sigmax))\n",
        "plt.show()"
      ],
      "id": "c9e92500",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3621bd7f"
      },
      "source": [
        "### Conditional probability distributions"
      ],
      "id": "3621bd7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0bb1c2"
      },
      "source": [
        "Manipulating the higher-order covariance matrix to obtain the covariance matrix for a lower-order conditional probability distribution is a bit more complicated, but still relatively simple.\n",
        "\n",
        "Let's take the bivariate probability distribution as an example. Imagine that we know the value of $y$ and would like to know the probability distribution for $x$ once $y$ is fixed. One way to do this is to fix the value of $y$ in the equation for the bivariate distribution (equivalent to taking a horizontal slice of the bivariate distribution), then work out the normalisation factor that is required to make the integral of the probability distribution for $x$ equal to one, as it should be for a proper probability distribution. Looking at the contour plot above, we can see, for instance, that if the value of $y$ was fixed to 1, the expected value of $x$ would be closer to -1 than to 0.\n",
        "\n",
        "Those manipulations can be shortcut by the following procedure. To obtain the variance of $x$ conditional on a particular value of $y$, we take its original variance, $\\sigma_x^2$, and subtract the variance accounted for by fixing $y$, which turns out to be $\\sigma_{xy}\\frac{1}{\\sigma_y^2} \\sigma_{xy}$ (written that way for a reason that will become clearer in a moment). Perhaps surprisingly, and helpfully, the conditional variance doesn't depend on the value to which $y$ is fixed.\n",
        "\n",
        "$\\sigma_{x|y}^2 = \\sigma_x^2 - \\sigma_{xy} \\frac{1}{\\sigma_y^2} \\sigma_{xy}$\n",
        "\n",
        "You can see that, the higher the correlation between the fixed variable and the remaining variable, the more the variance in the remaining random variable is reduced by knowing the fixed variable. This is true, as well, of higher-order probabilities with multiple fixed and remaining random variables.\n",
        "\n",
        "The new expected value for $x$ is given by a related expression: \n",
        "\n",
        "$\\mu_{x|y} = \\mu_x + \\sigma_{xy} \\frac{1}{\\sigma_y^2} (y - \\mu_y)$"
      ],
      "id": "0c0bb1c2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcd7d33f"
      },
      "source": [
        "# Define parameters for original bivariate distribution in contour plot above\n",
        "mux = muy = 0\n",
        "sigmax = 1.5\n",
        "varx = sigmax**2\n",
        "sigmay = 1\n",
        "vary = sigmay**2\n",
        "corrxy = -0.5\n",
        "covarxy = sigmax * sigmay * corrxy\n",
        "\n",
        "# Fix value of y, and define variance and expected value for x conditional on y\n",
        "y = 1\n",
        "varx_new = varx - (covarxy/sigmay)**2\n",
        "sigmax_new = np.sqrt(varx_new)\n",
        "mux_new = mux + (covarxy/sigmay)*(y - muy)\n",
        "x = np.linspace(mux_new - 3*sigmax_new, mux_new + 3*sigmax_new, 100)\n",
        "# Plot marginal distribution and conditional distribution\n",
        "plt.plot(x, stats.norm.pdf(x, mux, sigmax))\n",
        "plt.plot(x, stats.norm.pdf(x, mux_new, sigmax_new))"
      ],
      "id": "fcd7d33f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4be111b0"
      },
      "source": [
        "You can see in this plot that, as expected, the conditional distribution (orange) is slightly sharper than the marginal distribution (blue; the range of $x$ values is narrower and the peak is higher), because knowing the value of $y$ has reduced the variance in $x$.\n",
        "\n",
        "This is even more useful when it is generalised to higher order multivariate distributions, where several variables are fixed at once and the remaining variables obey a multivariate normal distribution. It's easiest first to reorder the variables so that the fixed variables are at the end of the list and the remaining variables at the front. Once that is done, the covariance matrix can be partitioned into four blocks: smaller covariance matrices for the still-unknown variables (top left) and the known variables (bottom right), and two diagonal blocks with the covariances between the unknown and known variables.\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \n",
        "\\begin{bmatrix} \\boldsymbol{\\Sigma}_{11} & | & \\boldsymbol{\\Sigma}_{12} \\\\\n",
        "\\text{---} & | & \\text{---} \\\\\n",
        "\\boldsymbol{\\Sigma}_{21} & | & \\boldsymbol{\\Sigma}_{22} \\end{bmatrix}$\n",
        "\n",
        "The process used to derive the conditional probability distribution is a generalisation of the procedure above for turning a bivariate distribution into a conditional univariate distribution. The new covariance matrix for the remaining unknown variables is given by the following:\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{1|2} = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12}\\,\\boldsymbol{\\Sigma}_{22}^{-1}\\,\\boldsymbol{\\Sigma}_{21}$,\n",
        "where (because covariance matrices are symmetric) $\\boldsymbol{\\Sigma}_{21} = \\boldsymbol{\\Sigma}_{12}^T$.\n",
        "\n",
        "The new vector of expected values for the remaining variables is given by the following:\n",
        "\n",
        "$\\boldsymbol{\\mu}_{1|2} = \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12}\\,\\boldsymbol{\\Sigma}_{22}^{-1}\\,(\\mathbf{x}_2-\\boldsymbol{\\mu}_2)$,\n",
        "where $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$ are the expected values of the remaining and fixed variables from the original distribution, and $\\mathbf{x}_2$ is the vector of fixed values. The updated covariance matrix and expected value vector can now be used in the multivariate normal distribution to give us the conditional probability of the $\\mathbf{x}_1$ subset of the variables given known values for the $\\mathbf{x}_2$ subset. This conditional probability is typically denoted as $p(\\mathbf{x}_1;\\mathbf{x}_2)$ or $p(\\mathbf{x}_1|\\mathbf{x}_2)$.\n",
        "\n",
        "The concepts we've been looking at are illustrated below by transforming a 4-dimensional multivariate normal distribution into 2-dimensional marginal distributions as well as a number of 2-dimensional conditional distributions corresponding to different values for the fixed variables. \n",
        "\n",
        "You could try playing with the starting parameters, but you have to make sure that the starting covariance matrix corresponds to a physically sensible probability distribution. The eigenvalues of the covariance matrix are the variances in the directions of the eigenvectors and they all have to be positive to be physically sensible. That means that the determinant of the matrix (which is the product of the eigenvalues) has to be positive."
      ],
      "id": "4be111b0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41afe492"
      },
      "source": [
        "# Define 4x4 covariance matrix in terms of standard deviations (sigma) of \n",
        "# individual variables and their correlations (rho). Some choices of the \n",
        "# correlation parameters are not allowed because they will not lead to a \n",
        "# self-consistent covariance matrix. One obvious example of inconsistency is if \n",
        "# x1 and x2 are both highly positively correlated to x3, but the correlation \n",
        "# between x1 and x2 is highly negative; this can be detected by the determinant\n",
        "# of the covariance matrix being negative. We will also avoid complications of \n",
        "# perfectly correlated variables (or linear combinations of variables), which\n",
        "# lead to a determinant of zero.\n",
        "sigma1 = 1.5\n",
        "sigma2 = 0.8\n",
        "sigma3 = 1.2\n",
        "sigma4 = 2.\n",
        "var1 = sigma1**2\n",
        "var2 = sigma2**2\n",
        "var3 = sigma3**2\n",
        "var4 = sigma4**2\n",
        "# Correlations have to be in the range -1 to 1, not including the endpoints\n",
        "rho12 = 0.5\n",
        "rho13 = -0.7\n",
        "rho14 = 0.3\n",
        "rho23 = -0.6\n",
        "rho24 = 0.4\n",
        "rho34 = -0.6\n",
        "# Covariances defined in terms of correlation between two variables and their\n",
        "# standard deviations\n",
        "covar12 = rho12 * sigma1 * sigma2\n",
        "covar13 = rho13 * sigma1 * sigma3\n",
        "covar14 = rho14 * sigma1 * sigma4\n",
        "covar23 = rho23 * sigma2 * sigma3\n",
        "covar24 = rho24 * sigma2 * sigma4\n",
        "covar34 = rho34 * sigma3 * sigma4\n",
        "\n",
        "# For convenience, define expected values in terms of 2 arrays of length 2\n",
        "mu1 = np.array([1.,-1.])\n",
        "mu2 = np.array([2.,0.])\n",
        "#\n",
        "Sigma = np.array([[   var1, covar12, covar13, covar14],\n",
        "                  [covar12,    var2, covar23, covar24],\n",
        "                  [covar13, covar23,    var3, covar34],\n",
        "                  [covar14, covar24, covar34,    var4]])\n",
        "mu = np.concatenate((mu1,mu2))\n",
        "\n",
        "# Check covariance matrix for consistency before proceeding\n",
        "det = linalg.det(Sigma)\n",
        "if (det > 0):\n",
        "    # Start by plotting marginal joint distributions of x1/x2 and x3/x4\n",
        "    fig1, ax1 = plt.subplots(1,2)\n",
        "    fig1.set_dpi(150)\n",
        "    \n",
        "    # x1/x2 marginal joint distribution\n",
        "    ax1[0].set_aspect(\"equal\")\n",
        "    ax1[0].set_title(\"Marginal for x1 and x2\")\n",
        "    # Make a 2-dimensional grid over variables x and y\n",
        "    x1, y1, pos1 = make_grid(mu1,np.array((sigma1,sigma2)))\n",
        "    Sigma11 = Sigma[0:2,0:2]\n",
        "    # Add the eigenvectors showing the principal axes\n",
        "    # Note that eigenvectors are in the columns, and the eigenvalues are the variances of those components\n",
        "    w,v = linalg.eig(Sigma11)\n",
        "    end1 = mu1 + np.sqrt(w[0])*v[:,0] # Square root to show standard deviation\n",
        "    annotate_arrow(ax1[0], mu1, end1)\n",
        "    end2 = mu1 + np.sqrt(w[1])*v[:,1] # Square root to show standard deviation\n",
        "    annotate_arrow(ax1[0], mu1, end2)\n",
        "    bivardist = stats.multivariate_normal(mu1,Sigma11)\n",
        "    ax1[0].contourf(x1, y1, bivardist.pdf(pos1))\n",
        "    \n",
        "    # x3/x4 marginal joint distribution, with grid of fixed points used below for conditional probabilities\n",
        "    ax1[1].set_aspect(\"equal\")\n",
        "    ax1[1].set_title(\"Marginal for x3 and x4 with markers\")\n",
        "    x2m, y2m, pos2m = make_grid(mu2,np.array((sigma3,sigma4)))\n",
        "    Sigma22 = Sigma[2:4,2:4]\n",
        "    w,v = linalg.eig(Sigma22)\n",
        "    eigvec1_scaled = np.sqrt(w[0])*v[:,0]\n",
        "    eigvec2_scaled = np.sqrt(w[1])*v[:,1]\n",
        "    count = -1\n",
        "    xsample = np.zeros(9)\n",
        "    ysample = np.zeros(9)\n",
        "    for i in range(-1,2):\n",
        "        for j in range(-1,2):\n",
        "            count += 1\n",
        "            this_vec = mu2 + (i*eigvec1_scaled + j*eigvec2_scaled)\n",
        "            xsample[count] = this_vec[0]\n",
        "            ysample[count] = this_vec[1]\n",
        "    end1 = mu2 + np.sqrt(w[0])*v[:,0] # Square root to show standard deviation\n",
        "    annotate_arrow(ax1[1], mu2, end1)\n",
        "    end2 = mu2 + np.sqrt(w[1])*v[:,1] # Square root to show standard deviation\n",
        "    annotate_arrow(ax1[1], mu2, end2)\n",
        "    bivardist = stats.multivariate_normal(mu2,Sigma22)\n",
        "    ax1[1].contourf(x2m, y2m, bivardist.pdf(pos2m))\n",
        "    ax1[1].scatter(xsample,ysample,color=\"grey\")\n",
        "    \n",
        "    # Prepare for conditional probability of x1/x2 given x3/x4\n",
        "    Sigma12 = Sigma[0:2,2:4]\n",
        "    Sigma21 = Sigma12.transpose()\n",
        "    Sigma22_inv = linalg.inv(Sigma22)\n",
        "    Sigma11_new = Sigma11 - Sigma12.dot(Sigma22_inv).dot(Sigma21)\n",
        "    sigma1_new = np.sqrt(Sigma11_new[0,0])\n",
        "    sigma2_new = np.sqrt(Sigma11_new[1,1])\n",
        "    rho12_new = Sigma11_new[0,1]/(sigma1_new*sigma2_new)\n",
        "    print(\"Changed parameters for marginal probability of x1 and x2:\")\n",
        "    print(\"  sigma1 has changed from \",sigma1,\" to \",sigma1_new)\n",
        "    print(\"  sigma2 has changed from \",sigma2,\" to \",sigma2_new)\n",
        "    print(\"  rho12 has changed from \",rho12, \" to \",rho12_new)\n",
        "    fig2, ax2 = plt.subplots(3,3,sharex=True,sharey=True)\n",
        "    fig2.set_dpi(150)\n",
        "    fig2.suptitle(\"Conditional distributions for x1 and x2 at markers\")\n",
        "    regression_matrix = Sigma12.dot(Sigma22_inv)\n",
        "    count = -1\n",
        "    for i in range(-1,2):\n",
        "        for j in range(-1,2):\n",
        "            ind1 = i + 1\n",
        "            ind2 = j + 1\n",
        "            count += 1\n",
        "            xy = np.array([xsample[count],ysample[count]])\n",
        "            prod = regression_matrix.dot(xy)\n",
        "            mu1_new = mu1 + Sigma12.dot(Sigma22_inv).dot(xy - mu2)\n",
        "\n",
        "            # x1/x2 conditional joint distributions\n",
        "            ax2[ind1,ind2].set_aspect(\"equal\")\n",
        "            ax2[ind1,ind2].set_title(\"i=\" + str(i) + \", j=\" + str(j))\n",
        "            bivardist = stats.multivariate_normal(mu1_new,Sigma11_new)\n",
        "            ax2[ind1,ind2].contourf(x1, y1, bivardist.pdf(pos1)) # Use grid from marginal probability\n",
        "    \n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Covariance matrix is non-positive definite with det = \",det)"
      ],
      "id": "41afe492",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLxOEfzB_MAX"
      },
      "source": [
        "Remember, we saw earlier that a conditional probability can be obtained by fixing the values of some variables (to make a slice of the higher dimensional probability) and then re-normalising so that the integral over all possible values is one. So the shape of the 2D conditional probability distributions above is the same as the shape of 2D slices in a 4D space (much harder to visualise!), just differing in the normalisation scale factor."
      ],
      "id": "aLxOEfzB_MAX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20fd57ef"
      },
      "source": [
        "## Optional extra: the multiplication law of probabilities and Bayes' theorem"
      ],
      "id": "20fd57ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d87af59"
      },
      "source": [
        "As a short digression, we can demonstrate a very important feature of joint probability distributions with conditional and marginal probability distributions like the ones we derived above. This applies to any conditional and marginal probabilities, not just multivariate normal distributions.\n",
        "\n",
        "Consider the joint distribution of $x$ and $y$ shown above. We saw how we can transform it into either a marginal probability or a conditional probability. For the marginal probability, we could have integrated over all values of $x$ instead of $y$ to get the marginal probability distribution for $y$. Going back to the conditional probability of $x$ given $y$, we can think of that as being a renormalised line sliced from the joint distribution along a particular value of $y$. To renormalise it, we integrate along that line, then divide the values by that integral so that a new integral will come to one (as required for a sensible probability distribution that covers all possible outcomes). Notice that the integral along the line is the same operation we carried out to get the marginal probability of $y$. So what this tells us is that our recipe for fixing the value of one variable and renormalising to get the conditional probability can be expressed with this equation:\n",
        "\n",
        "$p(x;y)=\\frac{p(x,y)}{p(y)}$\n",
        "\n",
        "Rearranging gives us the multiplication law in which the product of conditional and marginal probabilities gives us the joint probability:\n",
        "\n",
        "$p(x,y) = p(x;y)\\,p(y)$.\n",
        "\n",
        "If we notice that the joint probability can be expressed in two ways, we can easily derive Bayes' theorem.\n",
        "\n",
        "$p(x,y) = p(x;y)\\,p(y) = p(y;x)\\,p(x)$\n",
        "\n",
        "$p(x;y) = \\frac{p(y;x)\\,p(x)}{p(y)}$\n",
        "\n",
        "This expression is extremely useful in the common situations where it's much easier to derive the conditional probability one way around, but we really want the other conditional probability.\n",
        "\n",
        "Of course, instead of $x$ and $y$ we can just as well have probabilities involving vectors of multiple variables, $\\mathbf{x}_1$ and $\\mathbf{x}_2$."
      ],
      "id": "6d87af59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3df7999d"
      },
      "source": [
        "## Probability distributions of sums and linear combinations of random variables"
      ],
      "id": "3df7999d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc84c5f"
      },
      "source": [
        "### Sums of random variables"
      ],
      "id": "cfc84c5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15fce326"
      },
      "source": [
        "We've already seen that if you add two independent variables, each of which follows a normal distribution, the sum will also be a normal distribution with an expected value given by the sum of the expected values and a variance given by the sum of the variances. Let's assume for now that the sum of two random variables that are correlated to each other, but still obey a bivariate normal distribution, will also follow a normal distribution. (We'll come back to this later.)  \n",
        "\n",
        "Take the case $z=x+y$, where $x$ and $y$ are related by a bivariate normal distribution. To define the normal distribution governing $z$, we need its mean and its variance.\n",
        "\n",
        "$\\left<z\\right> = \\iint{p(x,y)\\,(x+y)\\,dx\\,dy} = \\left<x\\right> + \\left<y\\right>$\n",
        "\n",
        "because the expected value of a sum is the sum of the expected values. To determine the variance, we start by expanding the equation for the variance of $z$ in terms of $x$ and $y$, taking the expected value of each term in the sum.\n",
        "\n",
        "$\\sigma_z^2 = \\left<(z-\\left<z\\right>)^2\\right> = \\left<\\Big((x-\\left<x\\right>)+(y-\\left<y\\right>)\\Big)^2\\right>\n",
        "= \\left<(x-\\left<x\\right>)^2\\right> + 2\\left<(x-\\left<x\\right>)(y-\\left<y\\right>)\\right>+ \\left<(y-\\left<y\\right>)^2\\right>$ \n",
        "\n",
        "$\\sigma_z^2 = \\sigma_x^2 + \\sigma_y^2 + 2\\sigma_{xy}$\n",
        "\n",
        "Notice that this is equal to the sum of the elements of the covariance matrix describing the relationship between $x$ and $y$.\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma^2_x & \\sigma_{xy} \\\\ \\sigma_{xy} & \\sigma^2_y \\end{bmatrix}$\n",
        "\n",
        "In fact, this can be extended to multiple variables by summing all the covariance elements corresponding to pairs of terms in the sum. Consider a situation with 5 variables, $x_1$ through $x_5$ and the sums $u = x_1 + x_2 + x_3$ and $v = x_4 + x_5$. We can partition the 5x5 covariance matrix for $x_1$ through $x_5$, add up the terms in each partition, and easily obtain the covariance matrix relating $u$ and $v$.\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
        "\\sigma^2_{x_1}  & \\sigma_{x_1x_2} & \\sigma_{x_1x_3} & | & \\sigma_{x_1x_4} & \\sigma_{x_1x_5} \\\\\n",
        "\\sigma_{x_1x_2} & \\sigma^2_{x_2}  & \\sigma_{x_2x_3} & | & \\sigma_{x_2x_4} & \\sigma_{x_2x_5} \\\\\n",
        "\\sigma_{x_1x_3} & \\sigma_{x_2x_3} & \\sigma^2_{x_3}  & | & \\sigma_{x_3x_4} & \\sigma_{x_3x_5} \\\\\n",
        "\\text{---}      & \\text{---}      & \\text{---}      & | & \\text{---}      & \\text{---}      \\\\\n",
        "\\sigma_{x_1x_4} & \\sigma_{x_2x_4} & \\sigma_{x_3x_4} & | & \\sigma^2_{x_4}  & \\sigma_{x_4x_5} \\\\\n",
        "\\sigma_{x_1x_5} & \\sigma_{x_2x_5} & \\sigma_{x_3x_5} & | & \\sigma_{x_4x_5} & \\sigma^2_{x_5}\n",
        "\\end{bmatrix}$\n",
        "    \n",
        "$\\boldsymbol{\\Sigma}_{uv} = \\begin{bmatrix}\n",
        "\\sigma^2_u   & \\sigma_{uv} \\\\\n",
        "\\sigma_{uv}  & \\sigma^2_v \n",
        "\\end{bmatrix}$\n",
        "    \n",
        "where \n",
        "\n",
        "$\\sigma^2_u = \\sigma^2_{x_1} + \\sigma^2_{x_2} + \\sigma^2_{x_3} + 2\\sigma_{x_1x_2} + 2\\sigma_{x_1x_3} + 2 \\sigma_{x_2x_3}$\n",
        "\n",
        "$\\sigma^2_v = \\sigma^2_{x_4} + \\sigma^2_{x_5} + 2 \\sigma_{x_4x_5}$\n",
        "\n",
        "$\\sigma_{uv} = \\sigma_{x_1x_4} + \\sigma_{x_1x_5} + \\sigma_{x_2x_4} + \\sigma_{x_2x_5} + \\sigma_{x_3x_4} + \\sigma_{x_3x_5}$\n",
        "\n",
        "Let's get back to the question of why the sum of two correlated random variables will have a normal distribution. We can reach this conclusion by going back one step and constructing those variables from independent random variables. Consider the case of making two measurements of $x$ with independent Gaussian errors. Then we can say that $x_1 = x + \\delta_1$ and $x_2 = x + \\delta_2$. We can construct a covariance matrix that contains the underlying variable $x$ twice.\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
        "\\sigma_x^2 & 0 & | & \\sigma_x^2 & 0 \\\\\n",
        "0 & \\sigma_{\\delta_1}^2 & | & 0 & 0 \\\\\n",
        "\\text{---} & \\text{---} & \\text{---} & \\text{---} & \\text{---} \\\\\n",
        "\\sigma_x^2 & 0 & | & \\sigma_x^2 & 0 \\\\\n",
        "0 & 0 & | & 0 & \\sigma_{\\delta_2}^2\n",
        "\\end{bmatrix}$\n",
        "    \n",
        "Note that every off-diagonal covariance matrix element is zero, except for the ones relating $x$ to itself; the other pairs are all independent. Adding up the four quadrants of the covariance matrix gives:\n",
        "\n",
        "$\\sigma_{x_1}^2 = \\sigma_x^2 + \\sigma_{\\delta_1}^2$\n",
        "\n",
        "$\\sigma_{x_2}^2 = \\sigma_x^2 + \\sigma_{\\delta_2}^2$\n",
        "\n",
        "$\\sigma_{x_1x_2} = \\sigma_x^2$\n",
        "\n",
        "So the correlation between $x_1$ and $x_2$ is given by\n",
        "\n",
        "$\\rho_{x_1x_2} = \\frac{\\sigma_x^2}{\\sqrt{(\\sigma_x^2 + \\sigma_{\\delta_1}^2)(\\sigma_x^2 + \\sigma_{\\delta_2}^2)}}\n",
        "= \\frac{1}{\\sqrt{\\Big(1+\\frac{\\sigma_{\\delta_1}^2}{\\sigma_x^2}\\Big)\\Big(1+\\frac{\\sigma_{\\delta_2}^2}{\\sigma_x^2}\\Big)}}$\n",
        "  \n",
        "As expected, the larger the errors in $x_1$ and $x_2$ relative to their signal, the lower the correlation."
      ],
      "id": "15fce326"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ca64e6f"
      },
      "source": [
        "### Scaling variables"
      ],
      "id": "9ca64e6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59ea81b"
      },
      "source": [
        "We alluded above to scaling when we discussed normalising variables by dividing each by its standard deviation. Intuitively, we could see that the mean-square deviations of the normalised variables from their expected values will all be one. This kind of result is fairly straightforward to demonstrate a bit more formally.\n",
        "\n",
        "Recall the definition of an element of the covariance matrix:\n",
        "\n",
        "$\\sigma_{x_{ij}} = \\left<(x_i-\\mu_i)(x_j-\\mu_j)\\right>$\n",
        "\n",
        "Let $u_i = s_i\\,x_i$ and, of course, $\\sigma_{u_i} = s_i \\sigma_{x_i}$. In vector terms\n",
        "\n",
        "$\\mathbf{u} = \\mathbf{S}\\,\\mathbf{x}$, where $\\mathbf{S}$ is a diagonal matrix:\n",
        "\n",
        "$\\mathbf{S} = \\begin{bmatrix}\n",
        "s_1 & 0 & \\cdots & 0 \\\\\n",
        "0 & s_2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & s_n\n",
        "\\end{bmatrix}$\n",
        "                    \n",
        "We can use the outer product shorthand notation, discussed above, to express the whole covariance matrix for the vector $\\mathbf{x}$:\n",
        "\n",
        "$\\Sigma_x = \\left<(\\mathbf{x}-\\boldsymbol{\\mu}_x)(\\mathbf{x}-\\boldsymbol{\\mu}_x)^T\\right>$\n",
        "\n",
        "Then we can manipulate the similar expression for the covariance matrix for $\\mathbf{u}$.\n",
        "\n",
        "$\\Sigma_u = \\left<(\\mathbf{u}-\\boldsymbol{\\mu}_u)(\\mathbf{u}-\\boldsymbol{\\mu}_u)^T\\right>\n",
        "          = \\left<[\\mathbf{S}(\\mathbf{x}-\\boldsymbol{\\mu}_x)[\\mathbf{S}(\\mathbf{x}-\\boldsymbol{\\mu}_x)]^T\\right>\n",
        "          = \\left<\\mathbf{S}(\\mathbf{x}-\\boldsymbol{\\mu}_x)(\\mathbf{x}-\\boldsymbol{\\mu}_x)^T\\mathbf{S}\\right>$\n",
        "\n",
        "The last step used two facts:  $(\\mathbf{A}\\mathbf{b})^T = \\mathbf{b}^T\\mathbf{A}^T$, and tranposing a diagonal matrix doesn't change it. Finally, noting that the scale matrix is constant, so that it can be taken outside the expected value, we have:\n",
        "\n",
        "$\\Sigma_u = \\mathbf{S}\\left<(\\mathbf{x}-\\boldsymbol{\\mu}_x)(\\mathbf{x}-\\boldsymbol{\\mu}_x)^T\\right>\\mathbf{S} = \\mathbf{S}\\,\\Sigma_x\\,\\mathbf{S}$"
      ],
      "id": "b59ea81b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09580ede"
      },
      "source": [
        "### Linear combinations of variables"
      ],
      "id": "09580ede"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a3c4e3"
      },
      "source": [
        "Exactly the same manipulations can be applied to linear combinations of variables. In this case, a matrix $\\mathbf{A}$ of coefficients describing the linear combinations takes the place of the diagonal scale matrix $\\mathbf{S}$. To illustrate this, take the following case:\n",
        "\n",
        "$u_1 = 2 x_1 + 3 x_2 + 4 x_3$\n",
        "\n",
        "$u_2 = 1 x_1 + 5 x_2 + 2 x_3$\n",
        "\n",
        "$u_3 = 3 x_1 + 1 x_2 + 3 x_3$\n",
        "\n",
        "Expressing this in terms of vectors and a matrix gives\n",
        "\n",
        "$\\mathbf{u} = \\mathbf{A}\\mathbf{x}$, where\n",
        "\n",
        "$\\mathbf{A} = \\begin{bmatrix}\n",
        "2 & 3 & 4 \\\\\n",
        "1 & 5 & 2 \\\\\n",
        "3 & 1 & 3\n",
        "\\end{bmatrix}$ , *i.e.*\n",
        "\n",
        "$\\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "2 & 3 & 4 \\\\\n",
        "1 & 5 & 2 \\\\\n",
        "3 & 1 & 3\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
        "\n",
        "The covariance matrix for $\\mathbf{u}$ is then given by\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_u = \\mathbf{A}\\left<(\\mathbf{x}-\\boldsymbol{\\mu}_x)(\\mathbf{x}-\\boldsymbol{\\mu}_x)^T\\right>\\mathbf{A}^T = \\mathbf{A}\\boldsymbol{\\Sigma}_x\\mathbf{A}^T$\n",
        "\n",
        "The relevant difference with the case of scaling is that the transpose of the matrix of coefficients ($\\mathbf{A}^T$) is not generally equal to the original matrix ($\\mathbf{A}$). \n",
        "\n",
        "In fact, the number of linear combinations doesn't have to be equal to the number of components of $\\mathbf{x}$. If $\\mathbf{x}$ is of length $k_x$ and there are $k_u$ linear combinations specified by $\\mathbf{A}$, then $\\mathbf{A}$ will have dimensions $k_u$ by $k_x$, and $\\boldsymbol{\\Sigma}_u$ will be a symmetric matrix of dimension $k_u$.\n",
        "\n",
        "We will not go into detail, but one interesting special case is when the linear transformations correspond to the eigenvectors of the original covariance matrix. In this case, the basis set is transformed to the space of the eigenvectors, which are all linearly independent of each other, so that the new covariance matrix is diagonal."
      ],
      "id": "99a3c4e3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StrKIVUHv2rh"
      },
      "source": [
        "### Exercise on deriving the distribution of a sum of variables"
      ],
      "id": "StrKIVUHv2rh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJKVPpbkwCUR"
      },
      "source": [
        "Use what we have learned about probability distributions for linear combinations of variables to demonstrate the earlier result we obtained for sums of variables, in which the covariance elements for a sum are obtained by adding up the covariance elements in the block covering the variables making up the sum. You might find it easier to demonstrate this for a specific case (such as two sums of two from an initial set of four) than the general case!"
      ],
      "id": "UJKVPpbkwCUR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT5iT9i2qFZj"
      },
      "source": [
        "### Exercise on deriving marginal probability distributions"
      ],
      "id": "TT5iT9i2qFZj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWVurWTnqQE9"
      },
      "source": [
        "Again, use what we have learned about probability distributions for linear combinations of variables to demonstrate that the marginal distribution can be obtained simply by dropping rows, columns and vector elements corresponding to the variables that are being eliminated."
      ],
      "id": "KWVurWTnqQE9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd04df4"
      },
      "source": [
        "## Normal distributions of complex variables"
      ],
      "id": "bdd04df4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19272c3b"
      },
      "source": [
        "Here we will learn how to generalise what we've been learning to the complex numbers that play such an important role in crystallography. We will see that complex numbers have their own form of the normal distribution, which (for one complex number) looks like a circularly-symmetric bivariate normal distribution, except it is in the complex plane.\n",
        "\n",
        "The distributions of real variables are relevant to centric structure factors, which are real-valued, but not to acentric structure factors, which are complex-valued and outnumber the centric terms in macromolecular crystallography. If we want to work out probabilistic results for structure factors (particularly in devising likelihood functions), we need probability distributions of individual structure factors and collections of them. Fortunately, the Central Limit Theorem also applies to complex variables, with similar conditions on having a sufficient number of terms in a sum, none of which dominates the distribution.\n",
        "\n",
        "We can start with the Wilson distribution of structure factors, as a way of illustrating the complex normal distribution. If we assume that positions of the atoms in a crystal (specified in fractional coordinates as $\\mathbf{x}$) are distributed randomly in the unit cell, then the sums of the contributions of those atoms to structure factors will obey a complex normal distribution. (In fact, we only need to assume that the atoms are scattered randomly with respect to the Bragg planes corresponding to a particular *hkl*, because their phase angles, given by $2\\pi\\,\\mathbf{h}\\cdot\\mathbf{x}_j$ (modulo $2\\pi$), will be distributed randomly between 0 and $2\\pi$). Mathematically, the complex normal distribution looks very much like the normal distribution for real numbers, with the main difference being that the variance/covariance elements of the covariance matrix involve multiplying one complex number by the complex conjugate of the other. \n",
        "\n",
        "To see how the Central Limit Theorem applies, let's start with the contribution of an atom *j* placed randomly in the unit cell. The expected value of its contribution to a structure factor is 0, because that is the probability-weighted average of complex numbers lying on a circle around the origin of the complex plane. The variance of the contribution of that atom is obtained by the following integral over the phase angle $\\alpha = 2\\pi\\, \\mathbf{h} \\cdot \\mathbf{x}_j$ (where $\\frac{1}{2\\pi}$ is the probability of any particular phase):\n",
        "\n",
        "$\\int_0^{2\\pi}\\frac{1}{2\\pi}f_j\\exp(i\\alpha)\\,f_j\\exp(-i\\alpha)\\,d\\alpha = f_j^2$\n",
        "\n",
        "because the phase terms with opposite sign (complex conjugates) cancel.\n",
        "\n",
        "Note that the probability distribution for the contribution of one atom is non-zero only on the points on a circle, which doesn't look anything like a 2D normal distribution.  However, if we look at a structure factor obtained as the sum of contributions from a number of atoms:\n",
        "\n",
        "$\\mathbf{F}=\\sum_j f_j\\exp(2\\pi\\,i\\,\\mathbf{h}\\cdot\\mathbf{x}_j) $\n",
        "\n",
        "then the distribution of that sum will tend towards a complex normal distribution with an expected value of zero.\n",
        "\n",
        "$p(\\mathbf{F}) = \\frac{1}{\\pi\\Sigma_N}\\exp(-\\mathbf{F}^{*}\\frac{1}{\\Sigma_N}\\mathbf{F}) = \\frac{1}{\\pi\\Sigma_N}\\exp(-\\frac{|\\mathbf{F}|^2}{\\Sigma_N})$, where\n",
        "\n",
        "${\\Sigma_N} = \\left<|\\mathbf{F}|^2\\right> = \\sum_j f_j^2$\n",
        "\n",
        "We can look at the shape of this distribution (which is a circularly-symmetric 2D normal distribution in the complex plane) and how it is built up by random contributions from atoms. For simplicity, we will look at equal atoms with a scattering factor of 1, and see how the total structure factor is built up from contributions from 10 atoms.\n",
        "\n",
        "Note that every time you run the next cell, you get a different random structure factor from the distribution."
      ],
      "id": "19272c3b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fd90154"
      },
      "source": [
        "# Define Wilson distribution for acentric case (complex normal centered on zero)\n",
        "# Python doesn't have a complex normal distribution, but it can be treated as a\n",
        "# bivariate normal distributions with independent components of equal variance\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect(\"equal\")\n",
        "fig.set_dpi(150)\n",
        "ax.set_aspect(\"equal\")\n",
        "ax.set_title(\"Wilson distribution for 10 atoms with random sample\")\n",
        "\n",
        "SigmaN = 10.\n",
        "sigma1D = np.sqrt(SigmaN/2)\n",
        "Sigma = np.array([[SigmaN/2, 0],[0, SigmaN/2]])\n",
        "mu = np.array([0,0])\n",
        "\n",
        "x, y, pos = make_grid(np.array((0,0)),np.array((sigma1D,sigma1D))) # 2D grid for plotting\n",
        "complexdist = stats.multivariate_normal(mu,Sigma) # Define the distribution\n",
        "plt.contourf(x,y,complexdist.pdf(pos)) # Plot over grid\n",
        "\n",
        "start = np.array([0,0])\n",
        "for i in range(0,10):\n",
        "    random_angle = 2*np.pi*np.random.random_sample()\n",
        "    random_unit_vector = np.array([np.cos(random_angle),np.sin(random_angle)])\n",
        "    end = start + random_unit_vector\n",
        "    annotate_arrow(ax, start, end)\n",
        "    start = end\n",
        "    \n",
        "plt.show()"
      ],
      "id": "0fd90154",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "184c8252"
      },
      "source": [
        "### Exercise on generating a random sample in the complex plane"
      ],
      "id": "184c8252"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSo-26dXZtaq"
      },
      "source": [
        "Make a copy of the code above and change it so it doesn't draw arrows but instead generates a large sample of structure factors, each obtained by adding random contributions from 10 atoms, then representing each result as a dot."
      ],
      "id": "gSo-26dXZtaq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPg9pz4gaWmD"
      },
      "source": [
        "## The multivariate complex normal distribution"
      ],
      "id": "BPg9pz4gaWmD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7lgS69xajhx"
      },
      "source": [
        "The Wilson distribution is useful, but the power of the multivariate complex normal distribution in crystallography comes from considering what we learn from correlations among sets of structure factors. These are particularly useful for phasing, as well as understanding the distributions of intensity values observed in diffraction experiments. \n",
        "\n",
        "Some phasing examples: molecular replacement (MR) structure solution and phasing depends on what we learn about the true structure factor from a structure factor computed from a model; ensemble models for MR depend on what we learn if we have several alternative models; isomorphous replacement phasing depends on what we learn from the correlations among structure factors from a native crystal, a heavy-atom derivative crystal, and a model of the heavy-atom substructure; phasing using anomalous diffraction (SAD and MAD) depend on what we learn from Bijvoet pairs of structure factors when there are anomalous scatterers and we have a model for the substructure.\n",
        "\n",
        "The mathematical form of the multivariate complex normal distribution looks very much like the one for the multivariate normal distribution of real numbers, except for small changes like the ones we made to get the univariate complex normal distribution.\n",
        "\n",
        "$p(\\mathbf{z}) = (|\\pi\\boldsymbol{\\Sigma}|^{-1/2} \\exp(-(\\mathbf{z} - \\boldsymbol{\\mu})^H \\boldsymbol{\\Sigma}^{-1} (\\mathbf{z} - \\boldsymbol{\\mu}))$\n",
        "\n",
        "$\\boldsymbol{\\Sigma} = \\left<(\\mathbf{z} - \\boldsymbol{\\mu})(\\mathbf{z} - \\boldsymbol{\\mu})^H\\right>$  \n",
        "$= \\begin{bmatrix} \n",
        "\\left<(\\mathbf{z}_1 - \\boldsymbol{\\mu}_1)(\\mathbf{z}_1 - \\boldsymbol{\\mu}_1)^{*}\\right> & \\cdots & \\left<(\\mathbf{z}_1 - \\boldsymbol{\\mu}_1)(\\mathbf{z}_k - \\boldsymbol{\\mu}_k)^{*}\\right> \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\left<(\\mathbf{z}_1 - \\boldsymbol{\\mu}_1)(\\mathbf{z}_k - \\boldsymbol{\\mu}_k)^{*}\\right>^{*} & \\cdots & \\left<(\\mathbf{z}_k - \\boldsymbol{\\mu}_k)(\\mathbf{z}_k - \\boldsymbol{\\mu}_k)^{*}\\right> \n",
        "\\end{bmatrix}$\n",
        "\n",
        "In this equation we see the superscript $H$, indicating a Hermitian transpose. This differs from the normal transpose in taking the complex conjugate of each term at the same time as transposing the matrix or vector. As a result, the covariance matrix is no longer symmetric, but has Hermitian symmetry: element $ji$ is the complex conjugate of element $ij$. That can be seen in the representative off-diagonal terms in the expanded-out covariance matrix. In many cases, the complex covariances are real numbers so that, in practice, the complex covariance matrix is symmetric. However, if the relationship between two complex random variables involves a systematic phase shift, the corresponding covariance elements will indeed be complex numbers.\n",
        "\n",
        "Apart from these differences, the multivariate complex normal distribution behaves like the multivariate normal distribution for real numbers. In particular, the same manipulations are used to obtain conditional probability distributions or probability distributions for linear combinations of variables.\n",
        "\n",
        "With that background, we can look at a few examples from crystallography.\n"
      ],
      "id": "a7lgS69xajhx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obtc75PVcDub"
      },
      "source": [
        "### Statistical effect of symmetry on intensities "
      ],
      "id": "obtc75PVcDub"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4xoY_1UyjNN"
      },
      "source": [
        "Most space groups have different expected intensity factors for different classes of reflections. This arises because of the interference between Fourier terms from symmetry-related molecules: for some *hkl* values the average intensity is increased, whereas for others the intensity may be constrained to zero (systematic absence).\n",
        "\n",
        "To derive and understand the expected intensities, we only need to work with a covariance matrix, where the diagonal elements are the variances of the structure factors, which are in turn equivalent to the expected intensities.\n",
        "\n",
        "Consider a very simple space group, $P2$, where for every atom at position $x,y,z$, there is a symmetry-related atom at $-x,y,-z$. For simplicity, we'll just consider the case where there is one molecule per asymmetric unit. The contribution to the structure factor of the atoms in one molecule is the following:\n",
        "\n",
        "$\\mathbf{F}_1(h,k,l) = \\sum_j f_j \\exp(2\\pi\\,i\\,(hx_j+ky_j+lz_j)$\n",
        "\n",
        "If the molecule has no internal symmetry then (to a very good approximation) only Friedel symmetry ($\\mathbf{F}_1(h,k,l) = \\mathbf{F}_1(-h,-k,-l)^{*}$) applies and contributions to different *hkl* values are otherwise independent of each other. In $P2$, the contribution of the symmetry-related molecule is the following:\n",
        "\n",
        "$\\mathbf{F}_2(h,k,l) = \\sum_j f_j \\exp(2\\pi\\,i\\,(-hx_j+ky_j-lz_j) = \\mathbf{F}_1(-h,k,-l)$\n",
        "\n",
        "So we see that $\\mathbf{F}_1(h,k,l)$ and $\\mathbf{F}_2(h,k,l)$ are uncorrelated, except when $(h,k,l) = (-h,k,-l)$, *i.e.* when $h=l=0$. This means that, for the *0k0* class of reflections, the contributions of the two symmetry-related molecules are perfectly correlated.\n",
        "\n",
        "Since $\\mathbf{F}(h,k,l) = \\mathbf{F}_1(h,k,l) + \\mathbf{F}_2(h,k,l)$ we can work out the expected value of the intensity as a function of the reflection indices. The covariance matrix between $\\mathbf{F}_1(h,k,l)$ and $\\mathbf{F}_1(h,k,l)$ is the following:\n",
        "\n",
        "$\\begin{bmatrix}\\Sigma_N/2 & \\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right> \\\\\n",
        "\\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right>^{*} & \\Sigma_N/2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "If $h \\neq 0$ or $l \\neq 0$, the two symmetry-related contributions are uncorrelated ($\\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right> = 0$), but if $h$ and $k$ are both zero then the symmetry-related contributions are perfectly correlated ($\\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right> = \\Sigma_N/2$). Since the variance of $\\mathbf{F}$ (which is equivalent to the expected intensity) is the sum of the covariance elements, the expected intensity is $\\Sigma_N$ for the general reflections, but $2\\Sigma_N$ for the *0k0* reflections. In other words, the expected intensity factor for *0k0* reflections is two. Note in this case that there is no systematic phase shift between $\\mathbf{F}_1$ and $\\mathbf{F}_2$, so their covariances are real numbers and $\\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right> = \\left<\\mathbf{F}_1 \\mathbf{F}_2^{*}\\right>^{*}$.\n",
        "\n",
        "Systematic phase shifts arise in some space groups, with space group $P2_1$ being the simplest example. In this space group, atoms in the symmetry-related copy are found at $-x,\\frac{1}{2}+y,-z$. The contribution of the first copy to the structure factor is the same as in $P2$, but the screw translation changes the contribution of the second copy:\n",
        "\n",
        "$\\mathbf{F}_2(h,k,l) = \\sum_j f_j \\exp(2\\pi\\,i\\,(-hx_j+k(y_j+\\frac{1}{2})-lz_j) = \\mathbf{F}_1(-h,k,-l)\\exp(2\\pi\\,i\\frac{k}{2})$\n",
        "\n",
        "This means that, for *0k0* reflections where *k* is even, there is no phase shift and the expected intensity factor will be 2, as in the case of $P2$. However, for *0k0* reflections where *k* is odd, the two symmetry-related contributions will be out of phase by $\\pi$ or 180 degrees. In other words for the *k* odd *0k0* reflections, \n",
        "\n",
        "$\\mathbf{F}_2(0,k,0) = -\\mathbf{F}_1(0,k,0)$\n",
        "\n",
        "so they will cancel and the expected intensity factor is zero."
      ],
      "id": "D4xoY_1UyjNN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ufjuygQr6m"
      },
      "source": [
        "#### Advanced optional exercise: expected intensities in $P3_1$"
      ],
      "id": "X6ufjuygQr6m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk7z3JGBQ8pe"
      },
      "source": [
        "Work out the expected intensity of *00l* reflections in the space group $P3_1$, where atoms in the symmetry-related copies are found at $-y,x-y,\\frac{1}{3}+z$ and $-x+y,-x,\\frac{2}{3}+z$. \n",
        "\n",
        "There's a bit of a trick here: first express the symmetry-related coordinates in terms of a transformation matrix and a vector, *e.g.* for the second copy, and use the transpose operator explicitly instead of notation for the dot product.\n",
        "\n",
        "$\\mathbf{x}_2 = \\mathbf{S}_2 \\mathbf{x}_1 + \\mathbf{t}_2$\n",
        "\n",
        "$\\mathbf{F}_2(\\mathbf{h}) = \\sum_j f_j \\exp[2\\pi\\,i\\,\\mathbf{h}^T (\\mathbf{S}_2 (\\mathbf{x}_j + \\mathbf{t}_2))]$\n",
        "\n",
        "Then work out why this can alternatively be expressed in this way:\n",
        "\n",
        "$\\mathbf{F}_2(\\mathbf{h}) = \\sum_j f_j \\exp[2\\pi\\,i\\,(\\mathbf{S}_2^T\\,\\mathbf{h})^T (\\mathbf{x}_j + \\mathbf{t})]$\n",
        "\n",
        "which means that the symmetry operation in reciprocal space is the transpose of the symmetry operation in real space!\n",
        "\n",
        "Once you have done this, make sure you understand why the *00l* reflections are the ones with altered expected intensity factors. This involves the phase shifts resulting from the screw translations of the symmetry-related copies. How do the terms for the symmetry-related copies add up for different values of *l*?"
      ],
      "id": "Mk7z3JGBQ8pe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNWE2LWcR-Um"
      },
      "source": [
        "### Statistical effect of translational non-crystallographic symmetry (tNCS)"
      ],
      "id": "GNWE2LWcR-Um"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGl90gzYSCu_"
      },
      "source": [
        "We won't go into detail here, but the basic principles are the same as for the expected intensity factor arising from symmetry. If two copies of molecules within the asymmetric unit of a crystal are related by a pure translation, their contributions to the total structure factor will have the same magnitude but systematically different phases (depending on the dot product between *hkl* and the fractional translation vector). This has a statistical effect on the expected intensities. The situation is more complicated than the symmetry case because the translation vectors usually aren't rational numbers, and the tNCS-related copies usually differ somewhat in conformation or orientation, reducing the strength of the correlation terms. \n",
        "\n",
        "If you're interested, you can read more here: https://doi.org/10.1107/S0907444912045374."
      ],
      "id": "YGl90gzYSCu_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyeT0u0WTGil"
      },
      "source": [
        "### Conditional probability of the true structure factor given a model"
      ],
      "id": "OyeT0u0WTGil"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs9WXDJWTUbL"
      },
      "source": [
        "If the atomic model of a crystal bears any resemblance to the true crystal, the structure factors computed from the model will be correlated to the true structure factors. This gives us phase information, but it's useful to know how reliable that phase information is. For that reason, we're interested in knowing the conditional probability of the true structure factor given the structure factor computed from a model.\n",
        "\n",
        "For simplicity, we'll avoid symmetry and consider a crystal in space group $P1$. We'll start with one particularly simple case: where we have a perfect model for part of the structure but know nothing about the rest of the structure. This is a situation that was considered by Sim for the acentric case and by Woolfson for the centric case.\n",
        "\n",
        "Assuming that our molecule contains a sufficient number of atoms and none of them dominate, we can assume that both  structure factor contributions, from the modeled part and the missing part, are drawn from a complex normal distribution. The two contributions will, in general, be independent of each other (unless the two parts are similar *and* in a similar orientation). So we can easily make a covariance matrix for the joint distribution of these structure factor contributions. Traditionally, the known atoms are called the *P* atoms and the missing atoms the *Q* atoms. Here is the 2x2 covariance matrix with no correlations:\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{PQ} = \\begin{bmatrix}\n",
        "\\Sigma_P & 0 \\\\ 0 & \\Sigma_Q\n",
        "\\end{bmatrix}$\n",
        "\n",
        "We want to look at the relationship between the total structure factor:\n",
        "\n",
        "$\\mathbf{F}_N = \\mathbf{F}_P + \\mathbf{F}_Q$\n",
        "\n",
        "and just the $\\mathbf{F}_P$ component. To do this we can define a matrix $\\mathbf{A}$ that transforms $\\mathbf{F}_P$ and $\\mathbf{F}_Q$ into $\\mathbf{F}_P + \\mathbf{F}_Q$ ($=\\mathbf{F}_N$) and $\\mathbf{F}_P$.\n",
        "\n",
        "$\\mathbf{A} = \\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\end{bmatrix}$\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{NP} = \\mathbf{A} \\boldsymbol{\\Sigma}_{PQ} \\mathbf{A}^T$\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{NP} =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\Sigma_P & 0 \\\\ 0 & \\Sigma_Q\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\end{bmatrix}\n",
        "\\begin{bmatrix}\\Sigma_P & \\Sigma_P \\\\ \n",
        "\\Sigma_Q & 0 \\end{bmatrix}$\n",
        "\n",
        "$\\boldsymbol{\\Sigma}_{NP} =\n",
        "\\begin{bmatrix}\\Sigma_P + \\Sigma_Q & \\Sigma_P \\\\ \n",
        "\\Sigma_P & \\Sigma_P \\end{bmatrix} =\n",
        "\\begin{bmatrix}\\Sigma_N & \\Sigma_P \\\\ \n",
        "\\Sigma_P & \\Sigma_P \\end{bmatrix}$\n",
        "\n",
        "We can turn this into a correlation matrix by dividing every covariance matrix element by the square roots of the corresponding diagonal elements:\n",
        "\n",
        "$\\begin{bmatrix}1 & \\sqrt{\\frac{\\Sigma_P}{\\Sigma_N}} \\\\ \n",
        "\\sqrt{\\frac{\\Sigma_P}{\\Sigma_N}} & 1 \\end{bmatrix}$\n",
        "\n",
        "In other words, the complex correlation between $\\mathbf{F}_N$ and $\\mathbf{F}_P$ is the square root of the fraction of the total scattering accounted for by the model.\n",
        "\n",
        "Finally, to get the conditional distribution of $\\mathbf{F}_N$ given $\\mathbf{F}_P$, we need the conditional variance and expected values. Note that before we have a model, the expected values of both $\\mathbf{F}_N$ and $\\mathbf{F}_P$ are zero.\n",
        "\n",
        "$\\Sigma_{N|P} = \\Sigma_N - \\Sigma_P\\,\\Sigma_P^{-1}\\,\\Sigma_P = \\Sigma_N - \\Sigma_P = \\Sigma_Q$\n",
        "\n",
        "$\\left<\\mathbf{F}_N\\right>_{F_P} = 0 + \\Sigma_P\\,\\Sigma_P^{-1}\\,(\\mathbf{F}_P - 0) = \\mathbf{F}_P$\n",
        "\n",
        "$p(\\mathbf{F}_N;\\mathbf{F}_P) = \\frac{1}{\\pi\\Sigma_Q}\\exp(-\\frac{|\\mathbf{F}_N-\\mathbf{F}_P|^2}{\\Sigma_Q})$\n",
        "\n",
        "Compared to the Wilson distribution in that the centre of the distribution has moved from the origin to $\\mathbf{F}_P)$, and the distribution at that point is just the re-positioned Wilson distribution for $\\mathbf{F}_Q)$. This is illustrated below, with a randomly chosen value for $\\mathbf{F}_N$ (and the choice of $\\mathbf{F}_Q$ that implies) that you can regenerate by rerunning the cell."
      ],
      "id": "Xs9WXDJWTUbL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpn7eqU1vdHM"
      },
      "source": [
        "# Define Sim distribution for acentric case (complex normal centered on partial\n",
        "# structure factor)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect(\"equal\")\n",
        "fig.set_dpi(150)\n",
        "ax.set_aspect(\"equal\")\n",
        "ax.set_title(\"Sim distribution with Fp, random Fn and Fq(=Fn-Fp)\")\n",
        "\n",
        "# Define F_P as real, imaginary pair to provide to bivariate normal\n",
        "fp = np.array([15.,12.])\n",
        "SigmaQ = 100.\n",
        "sigma1D = np.sqrt(SigmaQ/2)\n",
        "Sigma = np.array([[SigmaQ/2, 0],[0, SigmaQ/2]])\n",
        "mu = fp\n",
        "\n",
        "xmin = min(0.,fp[0]-3*sigma1D)\n",
        "xmax = max(0.,fp[0]+3*sigma1D)\n",
        "ymin = min(0.,fp[1]-3*sigma1D)\n",
        "ymax = max(0.,fp[1]+3*sigma1D)\n",
        "print(xmin,xmax,ymin,ymax)\n",
        "centre = np.array([(xmin + xmax)/2, (ymin + ymax)/2])\n",
        "print(centre)\n",
        "scales = np.array([xmax-xmin,ymax-ymin]) / 6\n",
        "print(scales)\n",
        "x, y, pos = make_grid(centre,scales) # 2D grid for plotting\n",
        "complexdist = stats.multivariate_normal(mu,Sigma) # Define the distribution\n",
        "plt.contourf(x,y,complexdist.pdf(pos)) # Plot over grid\n",
        "\n",
        "start = np.array([0,0])\n",
        "end = np.random.multivariate_normal(mu,Sigma)\n",
        "annotate_arrow(ax, start, fp)\n",
        "annotate_arrow(ax, fp, end)\n",
        "annotate_arrow(ax, start, end)\n",
        "    \n",
        "plt.show()"
      ],
      "id": "Fpn7eqU1vdHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh08_arlzcdj"
      },
      "source": [
        "For real cases, we would need to consider the effect of coordinate errors (and B-factor errors and errors in identifying the correct atom type) on the accuracy of the calculated structure factor, but that would be beyond the scope of the time we have. In brief, errors in the model reduce the strength of the covariance terms. The case of random coordinate errors for a complete model was considered by Luzzati in the 1950s. To deal with this, we need to consider the expected value and variance of the contribution of an atom shifted from its correct position. The combination of model error and incompleteness was considered by Srinivasan and coworkers in the 1960s and 1970s."
      ],
      "id": "rh08_arlzcdj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLcJ7h-0rUQB"
      },
      "source": [
        "### Advanced optional exercise: multiple components and ensemble models in MR"
      ],
      "id": "hLcJ7h-0rUQB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCQN22--rn_x"
      },
      "source": [
        "Think about what would happen if you were either adding multiple components for different parts of the structure, or adding an ensemble of superimposed alternative models. What would the covariance matrix for the true structure factor and structure factors computed from the correctly-placed multiple models look like? What would the conditional probability distribution for the true structure factor given the collection of models look like in these cases? Why is it different when you add alternative models for the same thing compared to when you add models for different parts of the structure?"
      ],
      "id": "hCQN22--rn_x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a3zMetjf_X6"
      },
      "source": [
        "### Going further"
      ],
      "id": "5a3zMetjf_X6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkKdmdVcgD6X"
      },
      "source": [
        "All phasing methods can be approached starting by constructing covariance matrices relating appropriate sets of structure factors. The knowledge provided by models (and by having measurements of the amplitudes of the structure factors, not addressed here) restricts the possibilities for phase angles.\n",
        "\n",
        "Isomorphous replacement phasing can be approached by starting from the covariance matrix relating structure factors from a native crystal, one or more heavy-atom derivative crystals, and one or more models of the heavy-atom substructures.\n",
        "\n",
        "Phasing by anomalous dispersion requires looking at the Friedel pairs of reflections, which are highly correlated but which are not identical because of the effect of the anomalous scatterers.\n",
        "\n",
        "Constructing the appropriate multivariate complex normal distributions for different phasing cases is actually relatively straightforward compared to the other steps required to make a likelihood target for phasing and refinement. The likelihood target is the probability distribution for the set of measurements we made, given the parameters describing our model. Unfortunately, we are not able to measure the phase information directly, so all we have are intensities (with measurement errors), from which we can infer approximate values for the amplitudes of the structure factors. The phases, then, are *nuisance variables* that we have to integrate out from our structure factor probability distributions. This requires a change of variables and then an integration over every unknown phase angle, which (for multiple measurements) is a much more difficult problem, beyond the scope we can cover here!\n",
        "\n",
        "If you're interested, you could look at these papers:\n",
        "\n",
        "Likelihood in molecular replacement: https://doi.org/10.1107/S0907444901012471\n",
        "\n",
        "Likelihood in SAD phasing: https://doi.org/10.1107/S0907444904009990\n",
        "\n",
        "Likelihood-based methods in Phaser: https://doi.org/10.1107/s0021889807021206\n",
        "\n",
        "Likelihood in MIR phasing: https://doi.org/10.1107/s0907444903017918"
      ],
      "id": "hkKdmdVcgD6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va9tj18IVG7E"
      },
      "source": [
        "## Bonus optional exercise!\n",
        "If you spot some inelegant Python code (probably not a big challenge) and work out a more elegant way to do it, let me know!"
      ],
      "id": "va9tj18IVG7E"
    }
  ]
}